{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)\n",
    "\n",
    "\n",
    "## Introduction to Machine Learning\n",
    "### Scikit-Learn: Classification\n",
    "\n",
    "Author List: \n",
    "\n",
    "Alexander Fred Ojala, minor edits\n",
    "\n",
    "Sana Iqbal, significant edits for fall 2017\n",
    "\n",
    "Kevin Li, Ikhlaq Sidhu, Spring 2017\n",
    "\n",
    "Original Sources: http://scikit-learn.org,http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "License: Feel free to do whatever you want to with this code\n",
    "\n",
    "\n",
    "### Our  predictive machine learning models perform two types of tasks:\n",
    "\n",
    "* __CLASSIFICATION__:\n",
    "LABELS ARE DISCRETE VALUES.\n",
    "Here the model is trained to classify each instance into a set of predefined  discrete classes.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a  class of that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __defaulter or non-defaulter__ as labels. When we input income and expenditure data  of any customer in this model, it will predict whether the customer is going to default or not.\n",
    "\n",
    "* __REGRESSION__:\n",
    "LABELS ARE CONTINUOUS VALUES.\n",
    "Here the model is trained to predict a continuous value for each instance.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a continuous value  for  that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __ default amount__ as the label. This model when input with income and expenditure data of any customer will be able to predict the default amount the customer might end up with.\n",
    "\n",
    "\n",
    "* __TO GET STARTED:__:\n",
    "\n",
    "We will use python library -SCIKIT-LEARN for our classification and regression models.\n",
    "\n",
    "1. Install numpy, scipy, scikit-learn.\n",
    "\n",
    "2. Download the dataset provided and save it in your current working directory.\n",
    "\n",
    "3. In the following sections  you will:\n",
    "\n",
    "    3.1 Read the dataset into the python program.\n",
    "    \n",
    "    3.2 Look  into the dataset characteristics, check for feature type - categorical or numerical.\n",
    "    \n",
    "    3.3 Find feature distributions to check sufficiency of data.\n",
    "    \n",
    "    3.4 Divide the dataset into training and validation subsets.\n",
    "    \n",
    "    3.5 Fit models with training data  using scikit-learn library.\n",
    "    \n",
    "    3.6 Calculate training error.\n",
    "    \n",
    "    3.7 Test model prediction accuracy using validation data.\n",
    "    \n",
    "    3.8 Report model performance on validation data using different metrics.\n",
    "    \n",
    "    3.9 Save the model parameters in a pickle file so that it can be used for test data.\n",
    "    \n",
    "  Also, if our data set is small we will have fewer examples for validation. This will not give us a a good estimatiion of model error. We can use  k-fold crossvalidation in such situations. In k-fold cross-validation, the shuffled training data is partitioned into k disjoint sets and the model is trained on k âˆ’1 sets and validated on the kth set. This process is repeated k times with each set chosen as the validation set once. The cross-validation accuracy is reported as the average accuracy of the k iterations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with a little touch up.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example on some pandas features that we are going to use:\n",
    "# identifying any Null values in our dataframe,\n",
    "# replacing null values,\n",
    "# indexing dataframe,\n",
    "# shuffling\n",
    "\n",
    "'''\n",
    "example_df = pd.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n",
    "# Make a few areas have NaN (Not a Number) values\n",
    "example_df.iloc[3:,1] = np.nan\n",
    "example_df.iloc[2,0] = np.nan\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if value in a cell is NAN \n",
    "example_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total no of NANs in a column\n",
    "example_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nan with values\n",
    "example_df0=example_df.fillna(0)\n",
    "example_df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nan with values, USE inplace =True ,to change dataframe\n",
    "example_df1=example_df.fillna(example_df.mean())\n",
    "example_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to shuffle dataframe values\n",
    "from sklearn.utils import shuffle\n",
    "example_df_3= shuffle(example_df).reset_index(drop=True)\n",
    "example_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bin data values \n",
    "import math\n",
    "bins=[-math.inf,0,math.inf]\n",
    "labels=['negative','postive']\n",
    "example_df['label'] = pd.cut(example_df['C'], bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path='./data/iris_classification.csv'\n",
    "data=pd.read_csv(file_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHUFFLE data instances to randomize the distribution of different classes\n",
    "# Check if data has any NAN  values, you can choose to drop NAN \n",
    "# containing rows or replace NAN  values with mean. median,or any assumed value.\n",
    "\n",
    "data= shuffle(data).reset_index(drop=True)\n",
    "print('Number of NaNs in the dataframe:\\n',data.isnull().sum())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FEATURES / INPUT DATA\n",
    "X=data.iloc[:,:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET LABELS FROM THE DATA\n",
    "Y=data['species']\n",
    "print (Y.value_counts()) #gives the count of each label in the dataset\n",
    "print (Y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"We will map our class labels to integers and \\\n",
    "        then use in modeling.\\nThe mapping is:'versicolor': 0, \\\n",
    "        'virginica': 1,'setosa' :2 \\n\")\n",
    "\n",
    "Y=Y.map({'versicolor': 0, 'virginica': 1,'setosa' :2})\n",
    "print (Y.value_counts()) #gives the count of each label in the dataset\n",
    "\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature vector shape=\", X.shape)\n",
    "print(\"Class shape=\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get feature distribution of each feature\n",
    "data.hist(figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check feature distribution of each class, \n",
    "# to get an overview of feature and class relationshhip,\n",
    "# also useful in validating data\n",
    "\n",
    "data.groupby('species').hist(figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE SKLEARN INBUILT FUNCTION TO BUILD A LOGISTIC REGRESSION MODEL  \n",
    "For Details check :\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "\n",
    "In order to check the validity of our trained model, we keep a part of our dataset hidden from the model during training, called  __Validation data__.\n",
    "\n",
    "Validation data labels are predicted using the trained model and compared with the actual labels of the data.This gives us the idea about how well the model can be trusted for its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation set  using sklearn function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "            train_test_split(X, Y, test_size=0.2, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Name our regression object\n",
    "logreg_model = linear_model.LogisticRegression(C=1.0)\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "\n",
    "print ('Training a logistic Regression Model..')\n",
    "logreg_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ACCURACY\n",
    "\n",
    "training_accuracy=logreg_model.score(x_train,y_train)\n",
    "print ('Training Accuracy:',training_accuracy)\n",
    "\n",
    "# OR :\n",
    "\n",
    "\n",
    "# this line below will predict a category for every row in x_train\n",
    "Z = logreg_model.predict(x_train) \n",
    "\n",
    "\n",
    "# Estimate errors in an array called L\n",
    "def find_error(Y,Z):\n",
    "    '''Y:actual_labels\n",
    "    Z:predicted_labels'''\n",
    "    \n",
    "    L = np.arange(len(Y))\n",
    "    for i,value in enumerate(Y):\n",
    "        if value == Z[i]: \n",
    "            L[i] = 0\n",
    "        else:\n",
    "            L[i] = 1\n",
    "            \n",
    "    error_rate=np.average(L)\n",
    "\n",
    "    print (\"\\nThe error rate is \", error_rate)\n",
    "    print ('\\nThe accuracy of the model is ',1-error_rate )\n",
    "\n",
    "    print (\"Y-actual Z-predicted Error \\n\")\n",
    "    for i,value in enumerate(Y):\n",
    "        print (value, Z[i], L[i])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_error(y_train,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION ACCURACY: \n",
    "# we will find accuracy of the model \n",
    "# using data that was not used for training the model\n",
    "\n",
    "validation_accuracy=logreg_model.score(x_test,y_test,)\n",
    "print('Accuracy of the model on unseen validation data: ', \\\n",
    "      validation_accuracy)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = y_test\n",
    "y_pred = logreg_model.predict(x_test)\n",
    "cf=pd.DataFrame(confusion_matrix(y_true, y_pred),\\\n",
    "                columns=['Pred 0',1,2],index=['Act 0',1,2])\n",
    "print ('Confusion matrix of test data is: \\n')\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE DECISION BOUNDARIES:\n",
    "# 1.create meshgrid of all points between \n",
    "\n",
    "#For that we will create a mesh between [x_min, x_max]x[y_min, y_max].\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min = X['sepal_length'].min() - .5\n",
    "x_max =X['sepal_length'].max() + .5\n",
    "y_min = X['sepal_width'].min() - .5\n",
    "y_max = X['sepal_width'].max() + .5\n",
    "\n",
    "\n",
    "\n",
    "# print x_min, x_max, y_min, y_max\n",
    "xr = np.arange(x_min, x_max, h)\n",
    "yr = np.arange(y_min, y_max, h)\n",
    "\n",
    "xx, yy = np.meshgrid(xr, yr)\n",
    "Z = logreg_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "\n",
    "# Notes to help read code\n",
    "# numpy.ravel(a, order='C')  Return a contiguous flattened array. \n",
    "#â€˜Câ€™ means to index the elements in row-major, C-style order, with \n",
    "# the last axis index changing fastest, \n",
    "# back to the first axis index changing slowest.\n",
    "\n",
    "# numpy.c_ = <numpy.lib.index_tricks.CClass object at 0x49cad40c>\n",
    "# Translates slice objects to concatenation along the second axis.\n",
    "# >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
    "# provides: array([[1, 2, 3, 0, 0, 4, 5, 6]])\n",
    "\n",
    "print ('finished Z')\n",
    "\n",
    "# another approach is to make an array Z2 which has all \n",
    "# the predicted values in (xr,yr).  \n",
    "# This takes longer\n",
    "Z2 = np.arange(len(xr)*len(yr)).reshape(len(xr),len(yr))\n",
    "for yni in range(len(yr)):\n",
    "    for xni in range(len(xr)):\n",
    "#         print (xni, yni, logreg_model.predict([[xr[xni],yr[yni]]]))\n",
    "\n",
    "        Z2[xni,yni] =logreg_model.predict([[xr[xni],yr[yni]]])\n",
    "print ('finished Z2')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try these to better undertand the code above\n",
    "print (Z.shape)\n",
    "#print len(xr), len(yr)\n",
    "#print Z2.shape\n",
    "#Z2[[1,1]] = 2\n",
    "print (xni, yni)\n",
    "#print xx[0,01]\n",
    "#print yy[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "\n",
    "# Plot also the training points\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "# plt.xlabel('Sepal length')\n",
    "# plt.ylabel('Sepal width')\n",
    "\n",
    "# plt.xlim(xx.min(), xx.max())\n",
    "# plt.ylim(yy.min(), yy.max())\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.scatter(X['sepal_length'], X['sepal_width'], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.colorbar()\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot of decison boundary\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(x_train['sepal_length'], x_train['sepal_width'], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(5,5))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.colorbar()\n",
    "# Plot also the training points\n",
    "plt.scatter(x_test['sepal_length'], x_test['sepal_width'], c=logreg_model.predict(x_test), edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('VALIDATION DATA -PREDICTED LABELS')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.colorbar()\n",
    "label=np.unique(y_test)\n",
    "plt.title('VALIDATION DATA -ACTUAL LABELS')\n",
    "# Plot also the training points\n",
    "plt.scatter(x_test['sepal_length'], x_test['sepal_width'], c=y_test,label=np.unique(y_test), edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Example\n",
    "\n",
    "__Linear regression__ is a predictive modeling technique for predicting a numeric response variable based on features.  \n",
    "\"Linear\" in the name linear regression refers to the fact that this method fits a model where response bears linear relationship with features. (ie Z is proportional to first power of x)\n",
    "\n",
    "__Z = X0 + a(X1) + b(X2) +.... where:__   \n",
    "Z: predicted response  \n",
    "X0: intercept  \n",
    "a,b,..: Coefficients of X1,X2..  \n",
    "\n",
    "If Y is the actual response and Z is the predicted response,    \n",
    "__Y-Z= Residual__  \n",
    "Average Residual defines model performance,residual equal to zero represents a perfect fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Source: Scikit learn\n",
    "Code source: Jaques Grobler\n",
    "License: BSD 3 clause'''\n",
    "from sklearn import linear_model\n",
    "\n",
    "example_dff = pd.DataFrame(np.random.randint(0,100,size=(100, 1)),columns=['X'])\n",
    "example_dff['C']=5.1*example_dff['X']\n",
    "# example_dff['C']=5.1*example_dff['X']**2\n",
    "X_reg=example_dff[['X']]\n",
    "\n",
    "Y_reg=example_dff['C']\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_reg, Y_reg)\n",
    "Z_reg=regr.predict(X_reg)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\",np.mean((Z_reg - Y_reg) ** 2))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_reg['X'], Y_reg,  color='red')\n",
    "plt.plot(X_reg['X'], Z_reg, color='blue',\n",
    "         linewidth=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression using data with one feature -X')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break Out Section\n",
    "\n",
    "\n",
    "## Regression and Classification:\n",
    "__Data Source__:\n",
    "Datafile is in the data directory by name: __Energy.csv__\n",
    "\n",
    "The dataset was created by Angeliki Xifara ( Civil/Structural Engineer) and was processed by Athanasios Tsanas, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK).\n",
    "\n",
    "__Data Description__:\n",
    "\n",
    "The dataset contains eight attributes of a building (or features, denoted by X1...X8) and response being the heating load on the building, y1. \n",
    "\n",
    "* X1\tRelative Compactness \n",
    "* X2\tSurface Area \n",
    "* X3\tWall Area \n",
    "*  X4\tRoof Area \n",
    "*  X5\tOverall Height \n",
    "* X6\tOrientation \n",
    "*  X7\tGlazing Area \n",
    "*  X8\tGlazing Area Distribution \n",
    "*  y1\tHeating Load \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1:Read the data file in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.1.2: Describe data features in terms of type, distribution range and mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3: Plot feature distributions.This step should give you clues about data sufficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION\n",
    "### Q 2.1:  Bucket values of 'y1' i.e 'Heating Load'  in the original dataset into 3 classes: \n",
    "### 0:'Low' ( < 15),  1:'Medium'  (15-30),    2: 'High'  (>30)   \n",
    "### This converts the given dataset  into a classification problem, classes being *low, medium and high*.  \n",
    "### Use this datset for creating a  logistic regression classifiction model for predicting heating load type of a building. Use test-train split ratio of 0.15\n",
    "### Report training and test accuracies and  confusion matrices.\n",
    "\n",
    "\n",
    "HINT: Use pandas.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: One of the preprocessing steps in Data science is Feature Scaling i.e getting all our data on the same scale by setting same  Min-Max of feature values. This makes training less sensitive to the scale of features . Scaling is important in algorithms that use distance based classification, SVM or K means or involve gradient descent optimization . \n",
    "### If we  Scale features in the range [0,1] it is called unity based normalization. Perform unity based normalization on the above dataset and train the model again, compare model performance in training and validation with your previous model.\n",
    "refer:http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler  \n",
    "more at: https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  REGRESSION\n",
    "### Q3.1: Train a linear regression model on 85 percent of the given dataset, what is the intercept value and coefficient values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Q3.2: Report model performance using 'ROOT MEAN SQUARE' error metric on:  \n",
    "###   1. Data that was used for training(Training error)   \n",
    "###  2. On the 15 percent of unseen data (test error)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q4: Use varying data amounts  from your training data (100,200,300,400,500,all) to train models and report  training error and validation error.Plot error rates vs number of training examples.Do you see any relation.\n",
    "\n",
    "#### Hint: Shuffle data, convert to arrays, use array indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
